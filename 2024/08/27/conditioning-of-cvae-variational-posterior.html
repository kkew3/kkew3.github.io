<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Conditioning of the variational posterior in CVAE (Sohn, 2015) | Kaiwen’s personal website</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="Conditioning of the variational posterior in CVAE (Sohn, 2015)">
<meta property="og:locale" content="en_US">
<meta name="description" content="Abstract">
<meta property="og:description" content="Abstract">
<link rel="canonical" href="https://kkew3.github.io/2024/08/27/conditioning-of-cvae-variational-posterior.html">
<meta property="og:url" content="https://kkew3.github.io/2024/08/27/conditioning-of-cvae-variational-posterior.html">
<meta property="og:site_name" content="Kaiwen’s personal website">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-08-27T11:03:07+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Conditioning of the variational posterior in CVAE (Sohn, 2015)">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-08-27T11:03:07+00:00","datePublished":"2024-08-27T11:03:07+00:00","description":"Abstract","headline":"Conditioning of the variational posterior in CVAE (Sohn, 2015)","mainEntityOfPage":{"@type":"WebPage","@id":"https://kkew3.github.io/2024/08/27/conditioning-of-cvae-variational-posterior.html"},"url":"https://kkew3.github.io/2024/08/27/conditioning-of-cvae-variational-posterior.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="https://kkew3.github.io/feed.xml" title="Kaiwen's personal website">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2YQN8LEHLR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2YQN8LEHLR');
</script>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/">Kaiwen's personal website</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/docs/">Docs</a><a class="page-link" href="/about/">About</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Conditioning of the variational posterior in CVAE (Sohn, 2015)</h1>
    <span><a href="https://kkew3.github.io/tags/math--prob"><code class="highlighter-rouge"><nobr>math/probability</nobr></code></a> | <a href="https://kkew3.github.io/tags/ml--bayes"><code class="highlighter-rouge"><nobr>machine learning/bayesian</nobr></code></a></span>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-08-27T11:03:07+00:00" itemprop="datePublished">Aug 27, 2024 at 11:03:07
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="abstract">Abstract</h2>

<p>This post explores alternative conditioning of the variational posterior $q$ in CVAE (<a href="https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html">Sohn et al. 2015</a>), and concludes that the conditioning of $q$ on $y$ is important to predictive inference.</p>

<h2 id="introduction">Introduction</h2>

<p>CVAE models the likelihood $p(y \mid x)$ as a continuous mixture of latent $z$:</p>

<div id="eq-def"></div>

\[p(y \mid x) = \int p_\theta(z \mid x) p_\theta(y \mid x,z)\,\mathrm dz\,. \tag{1}\]

<p>Since (<a href="#eq-def">1</a>) is intractable, Sohn et al. instead optimize its evidence lower bound (ELBO):</p>

\[\mathcal L_{\text{CVAE}}(x,y;\theta,\phi) = \mathbb E_q[\log p_\theta(y \mid x,z) - \log q_\phi(z \mid x,y)]\,. \tag{2}\]

<p>Here, the variational posterior $q$ conditions on $x$ and $y$.
At test time, the authors propose to use importance sampling leveraging the trained variational posterior:</p>

<div id="eq-is"></div>

\[p(y \mid x) \approx \frac{1}{S} \sum_{s=1}^S \frac{p_\theta(y \mid x,z_s) p_\theta(z_s \mid x)}{q_\phi(z_s \mid x,y)}\,, \tag{3}\]

<p>where $z_s \sim q_\phi(z \mid x,y)$.</p>

<p>What if $q$ conditions on $x$ only?
This post explores this possibility, and reaches the conclusion that without conditioning on $y$, $q$ at optimum won’t ever attain the true posterior $p(z \mid x,y)$, and should not be otherwise better in terms of reducing the variance in importance sampling.</p>

<h2 id="warm-up-proving-the-effecacy-of-importance-sampling">Warm up: proving the effecacy of importance sampling</h2>

<p>We assume that infinite data is available for learning, and $q$ is from a flexible enough probability family.
The data are drawn from the joint data distribution $p_D(x,y)$, where we have stressed with a subscript $D$.
We assume that $x$ is continuous and $y$ is discrete.
The goal is to maximize the expected ELBO in terms of $p_D(x,y)$.
However, we assume that $p_\theta(y \mid x,z)$ won’t approaches to $p_D(y \mid x)$ whatever value $\theta$ picks.
We will drop $\theta$ and $\phi$ below for brevity.</p>

<p>We may easily pose this setup as a constrained maximization problem:
$\max \mathbb E[\log p(y,z \mid x) - \log q(z \mid x,y)]$ subject to $q$ being a probability, where the expectation is taken with respect to $p_D(x,y) q(z \mid x,y)$.</p>

<p>The Lagrangian is:</p>

<div id="eq-lagrangian-q-xy"></div>

\[\int \sum_y p_D(x,y) \int q(z \mid x,y) \log \frac{p(y,z \mid x)}{q(z \mid x,y)}\,\mathrm dz\,\mathrm dx + \int \sum_y \mu(x,y) \left(\int q(z \mid x,y)\,\mathrm dz - 1\right)\,\mathrm dx\,, \tag{4}\]

<p>where $\mu(x,y)$ is the Lagrange multiplier.
Now find the <a href="https://www.youtube.com/watch?v=6VvmMkAx5Jc&t=982s">Gateaux derivative</a> and let it equal zero:</p>

\[0 = p_D(x,y) (\log p(y,z \mid x) - (1 + \log q(z \mid x,y)) + \mu(x,y))\,.\]

<p>Absorbing $p_D(x,y) &gt; 0$ and the constant 1 into $\mu(x,y)$ yields:</p>

\[\log q(z \mid x,y) = \mu(x,y) + \log p(y,z \mid x)\,,\]

<p>where $\mu(x,y) = -\log \int p(y,z \mid x)\,\mathrm dz = -\log p_D(y \mid x)$.
It thus follows that, at optimum, $q(z \mid x,y) = p(z \mid x,y)$.
Hence, when evaluating Equation (<a href="#eq-is">3</a>), at optimum, the right hand side equals the left hand side with zero variance.</p>

<h2 id="conditioning-only-on-x-gives-worse-approximation">Conditioning only on x gives worse approximation</h2>

<p>Following the same setup as the previous section, we start from the Lagrangian (<a href="#eq-lagrangian-q-xy">4</a>).
Note that now we assume $q \triangleq q(z \mid x)$, and that the Lagrange multiplier is $\mu(x)$ instead of $\mu(x,y)$.
Rearranging the terms:</p>

\[\begin{multline}
    \int p_D(x) \int q(z \mid x) \left(\sum_y p_D(y \mid x) \log p(y,z \mid x) - \log q(z \mid x)\right)\,\mathrm dz\,\mathrm dz \\
    + \int \mu(x) \left(\int q(z \mid x)\,\mathrm dz - 1\right)\,\mathrm dx\,.
\end{multline}\]

<p>Let its Gateaux derivative with respect to $q$ equal zero:</p>

\[0 = p_D(x) \left(\sum_y p_D(y \mid x) \log p(y,z \mid x) - (1 + \log q(z \mid x))\right) + \mu(x)\,.\]

<p>Absorbing $p_D(x) &gt; 0$ and the constant 1 into $\mu(x)$ yields:</p>

\[\log q(z \mid x) = \mu(x) + \sum_y p_D(y \mid x) \log p(z \mid x,y) - \mathbb H(p_D(y \mid x))\,,\]

<p>where $\mathbb H(p_D(y \mid x)) = -\sum_y p_D(y \mid x) \log p_D(y \mid x)$ is the entropy.
We see immediately that:</p>

<div id="eq-main-result"></div>

\[q(z \mid x) \propto \exp(\mathbb E_{p_D(y \mid x)}[\log p(z \mid x,y)])\,. \tag{5}\]

<p>This means that when not conditioning on $y$, $q(z \mid x)$ can never achieve the true posterior $p(z \mid x,y)$, unless $\mathbb H(p_D(y \mid x)) = 0$, which is unlikely to occur.</p>

  </div>
<a class="u-url" href="/2024/08/27/conditioning-of-cvae-variational-posterior.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Kaiwen's personal website</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Kaiwen's personal website</li>
<li><a class="u-email" href="mailto:kps6326@hotmail.com">kps6326@hotmail.com</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list"><li><a href="https://github.com/kkew3"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">kkew3</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>My blogs and research reports.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
