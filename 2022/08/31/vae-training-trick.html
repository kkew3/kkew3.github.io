<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Variational Autoencoder training trick | Kaiwen’s personal website</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="Variational Autoencoder training trick">
<meta property="og:locale" content="en_US">
<meta name="description" content="A decent tutorial on Variational Autoencoder (VAE) can be found at arXiv. While I was playing with VAE, and trying the Gaussian output distribution with gradually higher dimensionality, I found a trick that ensure numerical stability at the beginning of training. As we know, the “encoder” of VAE outputs $\mu_X$ and $\log\sigma_X^2$ given input $x$, and a $z$ is sampled from the Gaussian determined by $\mu_X$ and $\sigma_X^2$ afterwards. To compute $\sigma_X^2$, $\sigma_X^2=e^{\log\sigma_X^2}$.">
<meta property="og:description" content="A decent tutorial on Variational Autoencoder (VAE) can be found at arXiv. While I was playing with VAE, and trying the Gaussian output distribution with gradually higher dimensionality, I found a trick that ensure numerical stability at the beginning of training. As we know, the “encoder” of VAE outputs $\mu_X$ and $\log\sigma_X^2$ given input $x$, and a $z$ is sampled from the Gaussian determined by $\mu_X$ and $\sigma_X^2$ afterwards. To compute $\sigma_X^2$, $\sigma_X^2=e^{\log\sigma_X^2}$.">
<link rel="canonical" href="https://kkew3.github.io/2022/08/31/vae-training-trick.html">
<meta property="og:url" content="https://kkew3.github.io/2022/08/31/vae-training-trick.html">
<meta property="og:site_name" content="Kaiwen’s personal website">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2022-08-31T11:48:35+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Variational Autoencoder training trick">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-31T11:48:35+00:00","datePublished":"2022-08-31T11:48:35+00:00","description":"A decent tutorial on Variational Autoencoder (VAE) can be found at arXiv. While I was playing with VAE, and trying the Gaussian output distribution with gradually higher dimensionality, I found a trick that ensure numerical stability at the beginning of training. As we know, the “encoder” of VAE outputs $\\mu_X$ and $\\log\\sigma_X^2$ given input $x$, and a $z$ is sampled from the Gaussian determined by $\\mu_X$ and $\\sigma_X^2$ afterwards. To compute $\\sigma_X^2$, $\\sigma_X^2=e^{\\log\\sigma_X^2}$.","headline":"Variational Autoencoder training trick","mainEntityOfPage":{"@type":"WebPage","@id":"https://kkew3.github.io/2022/08/31/vae-training-trick.html"},"url":"https://kkew3.github.io/2022/08/31/vae-training-trick.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="https://kkew3.github.io/feed.xml" title="Kaiwen's personal website">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2YQN8LEHLR"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2YQN8LEHLR');
</script>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/">Kaiwen's personal website</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
<a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/docs/">Docs</a><a class="page-link" href="/about/">About</a>
</div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Variational Autoencoder training trick</h1>
    <span><a href="https://kkew3.github.io/tags/ml"><code class="highlighter-rouge"><nobr>machine learning</nobr></code></a></span>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-08-31T11:48:35+00:00" itemprop="datePublished">Aug 31, 2022 at 11:48:35
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>A decent tutorial on Variational Autoencoder (VAE) can be found at <a href="https://arxiv.org/abs/1606.05908">arXiv</a>.
While I was playing with VAE, and trying the Gaussian output distribution with gradually higher dimensionality, I found a trick that ensure numerical stability at the beginning of training.
As we know, the “encoder” of VAE outputs $\mu_X$ and $\log\sigma_X^2$ given input $x$, and a $z$ is sampled from the Gaussian determined by $\mu_X$ and $\sigma_X^2$ afterwards.
To compute $\sigma_X^2$, $\sigma_X^2=e^{\log\sigma_X^2}$.</p>

<p>A problem arises, that $\log\sigma_X^2$ goes large enough such that $\sigma_X^2$ becomes floating-point infinity, especially when the mean and log variance is predicted by a dense linear layer and when the input dimension is high.
This is because, despite the fact that log variance is typically small at the end of training, it’s value at the beginning of training is determined by random initialization of the dense linear layer.
Suppose that the linear layer is initialized as standard Gaussian.
With $K$ input neurons, each output element of the linear layer follows the distribution of the sum of $K$ standard Gaussian, whose variance is positively proportional to $K$.
It follows that the maximum of all output elements is proportional to the variance.
Therefore, there should exist an element in $\sigma_X^2$ that is $e^K$ times the expected range.
Naturally, when $K$ is large, it goes to floating-point infinity.</p>

<p>To solve the problem, we may goes one step further.
Rather than predict $\log\sigma^2$, we predict $K\log\sigma_X^2$, and in turn the output variance becomes $e^{(K\log\sigma_X^2)/K}$, which won’t ever reach infinity.
Since $K$ is a constant throughout training, it won’t cause any effect to the training overall.</p>

  </div>
<a class="u-url" href="/2022/08/31/vae-training-trick.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Kaiwen's personal website</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Kaiwen's personal website</li>
<li><a class="u-email" href="mailto:kps6326@hotmail.com">kps6326@hotmail.com</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list"><li><a href="https://github.com/kkew3"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">kkew3</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>My blogs and research reports.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
